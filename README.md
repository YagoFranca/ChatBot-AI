# ğŸ¤– Chatbot de Atendimento para Marca de RelÃ³gios

Este projeto Ã© um assistente virtual inteligente desenvolvido em Python com Flask e LangChain. Ele Ã© especializado em atendimento ao cliente para uma renomada marca de relÃ³gios, oferecendo respostas sobre produtos, garantias, manutenÃ§Ãµes e suporte tÃ©cnico, alÃ©m de consultar informaÃ§Ãµes de atendimentos registrados em banco de dados.

---

## ğŸ§  Funcionalidades

- âœ… Suporte inteligente baseado em IA (modelo LLM via Ollama)
- ğŸ” Busca semÃ¢ntica de conhecimento com FAISS + embeddings
- ğŸ“ CorreÃ§Ã£o ortogrÃ¡fica automÃ¡tica (TextBlob + LanguageTool + pyspellchecker)
- ğŸ—ƒï¸ IntegraÃ§Ã£o com banco de dados SQLite para histÃ³rico de atendimentos
- ğŸ’¬ Suporte a conversas com memÃ³ria (LangChain Memory)
- ğŸŒ API HTTP com Flask para interaÃ§Ã£o via POST `/ask`
- ğŸ–¥ï¸ Interface amigÃ¡vel com **Streamlit**

---

## ğŸš€ Como executar localmente

### 1. Clone o repositÃ³rio


git clone https://github.com/seu-usuario/chatbot-relogios.git
cd chatbot-relogios

### 2. Crie e ative um ambiente virtual
bash
Copy
Edit
python -m venv .venv
source .venv/bin/activate        # Linux/macOS
.venv\Scripts\activate           # Windows

### 3. Instale as dependÃªncias
bash
Copy
Edit
pip install -r requirements.txt

###  4. Baixe e rode o modelo via Ollama (ex: llama3)
bash
Copy
Edit
ollama pull llama3
ollama serve

### ğŸ›  Como usar

### ğŸ“¡ Passo 1: Inicie o servidor de backend com Flask
bash
Copy
Edit
python src/app.py
O servidor estarÃ¡ disponÃ­vel em http://localhost:5000/ask.

### ğŸ’¬ Passo 2: Rode a interface do chatbot com Streamlit
bash
Copy
Edit
streamlit run chat.py
O chat.py se conecta ao servidor Flask via POST e permite interaÃ§Ã£o em tempo real com o assistente virtual.

